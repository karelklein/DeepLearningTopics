{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1 - Processing text to create design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                   delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data contains 25,000 reviews, each with a binary sentiment score and ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "['id' 'sentiment' 'review']\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): BeautifulSoup4 in /Users/Karel/anaconda/lib/python2.7/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a raw review, the function review_to_words will clean the text to remove HTML markup, stop words, and all punctuation and numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-z]\", \" \", review_text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can now be used to iterate through the training data and make a new list of cleaned reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the movie reviews training set...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Karel/anaconda/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/Karel/anaconda/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_reviews = train['review'].size\n",
    "clean_train_reviews = []\n",
    "sentiments_train = []\n",
    "\n",
    "print \"Cleaning and parsing the movie reviews training set...\\n\"\n",
    "for i in xrange(0, num_reviews):\n",
    "    clean_train_reviews.append(review_to_words(train['review'][i]))\n",
    "    sentiments_train.append(train['sentiment'][i])\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % ( i+1, num_reviews )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use clean_train_words to create a bag of words using sklearn's feature extraction methods. We limit the size of the vocabulary to 5000 words and create the X_counts array to house the raw words counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Creating the bag of words...\\n\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "X_counts = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now transform X_counts into a binary matrix X_binary, where counts greater than 0 become 1 and counts of 0 remain as they were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_binary = []\n",
    "for review in X_counts:\n",
    "    review_binary = []\n",
    "    for count in review:\n",
    "        if count > 0: \n",
    "            review_binary.append(1)\n",
    "        else:\n",
    "            review_binary.append(0)\n",
    "    X_binary.append(review_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sklearn's tfidf vectorizer to get the X_tfidf term-frequency inverse document-frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "X_tfidf = transformer.fit_transform(X_counts)\n",
    "X_tfidf = X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create X_binary_imabalnce that will delete 75% of reviews with sentiment=1 to create an imabalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_binary_imbalance = []\n",
    "X_1 = []\n",
    "for i in range(len(X_counts)):\n",
    "    if sentiments_train[i] == 1:\n",
    "        X_1.append(X_counts[i])\n",
    "    else:\n",
    "        X_binary_imbalance.append(X_counts[i])\n",
    "        \n",
    "# take X_1 and delete 75% of its rows randomly\n",
    "# first, select random indeces to keep\n",
    "\n",
    "random_indeces = np.random.choice(len(X_1), len(X_1)//4)\n",
    "for index in random_indeces:\n",
    "    X_binary_imbalance.append(X_1[int(index)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2 - Feature Space Similarity Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let dist(X, i, j, distance_function=’Euclidean’) be a function which returns the (Euclidean) distance between rows i and j of a design matrix. Import scipy to make the calculation efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(X,i,j,distance_function='Euclidean'):\n",
    "    matrix = [X[i],X[j]]\n",
    "    return scipy.spatial.distance.pdist(matrix, 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let topk(X, k) be a function which returns ((i1,j1,d1),...(ik,jk,dk)) where (ix,jx) are the indices of the xth closest pair, and dx is the corresponding distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topk(X,k):\n",
    "    X = csr_matrix(X)\n",
    "    distances = coo_matrix(euclidean_distances(X,X))\n",
    "    \n",
    "    tuples = izip(distances.row, distances.col, distances.data)\n",
    "    tup_sort = sorted(tuples, key=lambda x: (x[2]))\n",
    "    \n",
    "    # now get the first k unique elements from tup_sort\n",
    "    topk = []\n",
    "    i, skips = 0,0\n",
    "    while i < (k+skips):\n",
    "        if i > 0 and (tup_sort[i][0] == aaa[i-1][1])\\\n",
    "                and (tup_sort[i][1] == tup_sort[i-1][0]):\n",
    "            skips += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            topk.append(tup_sort[i])\n",
    "            i += 1\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assemble_topk(X,n,k):\n",
    "    segmentsize = len(X)//n\n",
    "    s = 0\n",
    "    winners = []\n",
    "    for i in np.arange(segmentsize,len(X)+segmentsize,segmentsize):\n",
    "        winners.append(topk(X[s:i],k))\n",
    "        s += segmentsize\n",
    "    return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(135, 356, 4.2426406871192848)], [(1269, 1548, 3.1622776601683795)], [(345, 621, 3.3166247903553998)], [(1538, 1745, 3.7416573867739413)], [(217, 517, 4.4721359549995796)], [(15, 2196, 4.4721359549995796)], [(116, 1369, 4.0)], [(989, 1328, 3.7416573867739413)], [(276, 2258, 2.2360679774997898)], [(1445, 1722, 3.3166247903553998)]]\n"
     ]
    }
   ],
   "source": [
    "print assemble_topk(X_counts,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#topk_X_binary = topk(X_binary,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#topk_X_binary_imblance = topk(X_binary_imbalance,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#topk_X_tfidf = topk(X_tfidf,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PART 3 - Classification Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
