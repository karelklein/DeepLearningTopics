{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief introduction to LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to begin talking about LSTMs, it is necessary to have a rudimentary understanding of Recurrent Neural Networks (RNNs). A RNN architecture utilizes the baseline neural network and constructs a loop with which it can retain information learned in prior events. By making this modification to the traditional NN, the RNN allows learning to be cumulative over a span of events. This difference provides a significant leap in predictive power: instead of learning how to classify an event from scratch upon every occurrence, a RNN can use the sequence leading to the current instance to make an informed decision about its class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of network has proven highly successful in applications including language modeling, image captioning, speech recognition, text prediction, and a host of others. One application familiar to the modern layman is the word suggestion feature on smartphones -- given a sequence of words typed, a new set of words is suggested, each a high-likelihood candidate for being the user's next choice. A RNN could be well employed in this setting.\n",
    "\n",
    "One of the drawbacks of the RNN, however, is its difficulty in capturing long-term dependencies. If the current task requires information from an event far in the past, a RNN will have likely perform poorly. To combat the issue, the LSTM steps in.\n",
    "\n",
    "A Long Short Term Memory (LSTM) unit is a type of recurrent neural network especially suited to learn from experiences with dependency gaps of unknown length. The LSTM unit implements a cell state, which is similar to a track across which information can flow easily across units.  Using gates made up a sigmoid neural net layer, the networks can regulate how much information to add or remove along the track. Once the information has been properly filtered, the cell state is put through another sigmoid layer that decides what parts to output. The cell state is finally pushed through *tanh* to map the value between *[-1,1]*, and multiplied by the output of the sigmoid to get the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tanh.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move to evaluating the model's proficiency for character-sequential prediction. In the next section, we will use a LSTM for a language classification experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs for Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will download some English and French text data from the Universal Declaration of Human Rights Database. To code the LSTM, we will use the keras library with TensorFlow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup & vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the texts in our local directory, we can read them in and lowercase all characters to simplify our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_eng = open('data/eng.txt').read().lower()\n",
    "text_frn = open('data/frn.txt').read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the data a bit further, I'll strip the end of line characters and make an array of the resulting space-separated strings. Now we can split each text set into a learning set containing 80% of the text, and a holdout set with the remaining 20%. Of course, these steps are done for both languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_eng_1 = text_eng.strip('\\n').split(' ')\n",
    "train_eng = text_eng_1[:len(text_eng_1)//10*8]\n",
    "test_eng = text_eng_1[len(text_eng_1)//10*8:]\n",
    "\n",
    "text_frn_1 = text_frn.strip('\\n').split(' ')\n",
    "train_frn = text_frn_1[:len(text_frn_1)//10*8]\n",
    "test_frn = text_frn_1[len(text_frn_1)//10*8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now re-join the arrays as strings in order to take note of some important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_eng = ' '.join(train_eng)\n",
    "x_test_eng = ' '.join(test_eng)\n",
    "\n",
    "x_train_frn = ' '.join(train_frn)\n",
    "x_test_frn = ''.join(test_frn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the English and French learning sets each contain around 9000 characters +/- 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus length:  8607\n",
      "French corpus length:  9529\n"
     ]
    }
   ],
   "source": [
    "print('English corpus length: ', len(x_train_eng))\n",
    "print('French corpus length: ', len(x_train_frn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now briefly join the two language sets in order to get the set of unique characters that define them. As we can see, there are 44 different characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  44\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(x_train_eng + x_train_frn)))\n",
    "print('total chars: ', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily retrive the index of a *char* given its character and vice-versa, we create a pair of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we iterate through the English and French texts and create a series 5-character slices (sub-strings) and track each substring's next character. By doing this, we build a learning set which the LSTM can use to learn its assigned task: character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cuttext(maxlen, text):\n",
    "    step = 1\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    return sentences, next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 8602\n",
      "nb sequences: 9524\n"
     ]
    }
   ],
   "source": [
    "sentences_eng, next_chars_eng = cuttext(5, x_train_eng)\n",
    "sentences_frn, next_chars_frn = cuttext(5, x_train_frn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure our model will be able to read the data, we will vectorize the sentences we made into a \"bag of chars\" binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize(sentences, maxlen, chars, char_indices, next_chars):\n",
    "    print('Vectorization...')\n",
    "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "        #y[i, char_indices[next_chars[i]]] = 1\n",
    "    return X #, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "X_eng, y_eng = vectorize(sentences_eng, 5, chars, char_indices,\\\n",
    "                         next_chars_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each of our N_samples has shape (5,44), meaning for each character in the string, there is a vector of length 44 where the corresponding char index is marked *True*, and all other indices are marked *False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8602, 5, 44) (8602, 44)\n"
     ]
    }
   ],
   "source": [
    "print(X_eng.shape, y_eng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "X_frn, y_frn = vectorize(sentences_frn, 5, chars, char_indices,\\\n",
    "                        next_chars_frn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation & training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build the LSTM. We will make both models of size 128, use a softmax for activation type, and categorical crossentropy for the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model for english...\n"
     ]
    }
   ],
   "source": [
    "print('Build model for english...')\n",
    "model_eng = Sequential()\n",
    "model_eng.add(LSTM(128, input_shape=(5, len(chars))))\n",
    "model_eng.add(Dense(len(chars)))\n",
    "model_eng.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model_eng.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model for french...\n"
     ]
    }
   ],
   "source": [
    "print('Build model for french...')\n",
    "model_frn = Sequential()\n",
    "model_frn.add(LSTM(128, input_shape=(5, len(chars))))\n",
    "model_frn.add(Dense(len(chars)))\n",
    "model_frn.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model_frn.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the two models in hand, we can now train them with our vectorized learning set, the corresponding vectorized next chars, 5 epochs and batch size of 128. Since our dataset size is relatively small, we can expect the computational burden common to LSTM training to take less of a toll here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8602/8602 [==============================] - 6s - loss: 0.5639     \n",
      "Epoch 2/5\n",
      "8602/8602 [==============================] - 7s - loss: 0.5557     \n",
      "Epoch 3/5\n",
      "8602/8602 [==============================] - 6s - loss: 0.5458     \n",
      "Epoch 4/5\n",
      "8602/8602 [==============================] - 6s - loss: 0.5471     \n",
      "Epoch 5/5\n",
      "8602/8602 [==============================] - 5s - loss: 0.5324     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133d18a90>"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eng.fit(X_eng, y_eng, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9524/9524 [==============================] - 6s - loss: 0.6387     \n",
      "Epoch 2/5\n",
      "9524/9524 [==============================] - 6s - loss: 0.6273     \n",
      "Epoch 3/5\n",
      "9524/9524 [==============================] - 6s - loss: 0.6197     \n",
      "Epoch 4/5\n",
      "9524/9524 [==============================] - 6s - loss: 0.6193     \n",
      "Epoch 5/5\n",
      "9524/9524 [==============================] - 6s - loss: 0.6084     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128717610>"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_frn.fit(X_frn, y_frn, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM prediction & testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the training complete, we look to generate a test dataset from the holdout sets. From each of the English and French sets, we randomly select 100 5-char substrings. We then end up with 200 test strings. These are the x_test for evaluation. The y_test are 1 for english and 0 for French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rands = np.random.randint(len(x_test_eng),size=100)\n",
    "eng_substrings = [x_test_eng[rands[i]:rands[i]+5] for i in range(100) if x_test_eng[rands[i]:rands[i]+5] != '']\n",
    "frn_substrings = [x_test_frn[rands[i]:rands[i]+5] for i in range(100) if x_test_frn[rands[i]:rands[i]+5] != '']\n",
    "\n",
    "x_test = np.concatenate((eng_substrings, frn_substrings))\n",
    "y_test = np.concatenate(([1]*100, [0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the current test set contains 200 length-5 strings, we create a new list of substrings that segments each string into its first *k* characters for a total of 5 new strings per original string. For example: *'shape'* is broken down into *'s','sh','sha','shap','shape'*.\n",
    "With this structure, we will be able to calculate the conditional probability of each character in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test_subs = []\n",
    "for s in x_test:\n",
    "    for i in range(len(s)):\n",
    "        x_test_subs.append(s[:i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we vectorize our new length 1000 test set so our model can accept it as input and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "x_test_subs_vect = vectorize(x_test_subs, 5, chars, char_indices, next_chars_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vectorized test set:  (1000, 5, 44)\n"
     ]
    }
   ],
   "source": [
    "print ('Shape of vectorized test set: ', x_test_subs_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use our English model to predict the next character for each of the strings in our test set. The result will be a (1000, 44) array, where for each string, a probability distribution is generated describing the likelihood of the next character. Since there are 44 characters in our set, each will be assigned a probability of being next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_eng = model_eng.predict(x_test_subs_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to calculate the log likelihood of a string for each model. To do this, we will move one character at a time, and calculate the probability of the next character given the prior sequence of characters of that string. An example of this computation would look like this for the English model on the word 'trump':\n",
    "\n",
    "*Pr(trump) = Pr(t|START) x Pr(r|START,t) x ... x Pr(p|START,trum)*\n",
    "\n",
    "Since we want the log likelihoods, we would take the sum of the log probabilities. We will run this process for both language models to get an estimate of the probability that the string was generated by each language. Unfortunately, the probability of trump is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getlogprobs(x_test_SUBS, prediction, char_indices):\n",
    "    all_log_probs = []\n",
    "    #words = []\n",
    "    for i in range(0,len(x_test_SUBS), 5):\n",
    "        \n",
    "        log_prob_sum = 0\n",
    "        prob_sum = 0\n",
    "\n",
    "        # get log probability of first char\n",
    "        first_char = x_test_SUBS[i]\n",
    "        prob_first_char = prediction[i][char_indices[first_char]]\n",
    "        log_prob_sum += np.log(prob_first_char)\n",
    "        if i < 10:\n",
    "            print ('P('+ first_char + '|START) = ' + str(prob_first_char))\n",
    "\n",
    "        # get log probability of next char given prior chars\n",
    "        for c in range(4):\n",
    "            true_next = x_test_SUBS[i + c + 1][-1:]\n",
    "            cond_prob = prediction[i + c][char_indices[true_next]]\n",
    "            if i < 10:\n",
    "                print ('P('+ true_next + '|'+ x_test_SUBS[i+c]+') = ' + str(cond_prob))\n",
    "\n",
    "            prob_sum += cond_prob\n",
    "            log_prob_sum += np.log(cond_prob)\n",
    "        if i < 10:\n",
    "            print ('-'*25, '\\n', 'P('+ x_test_SUBS[i+c+1]+ ') = ' + str(prob_sum)+'\\n'+'-'*25)\n",
    "        all_log_probs.append(log_prob_sum)\n",
    "        #words.append(x_test_subs[i+c+1])\n",
    "    return all_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will log first few calculations of conditional probabilities to get an intuitive sense of how the process works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P( |START) = 0.0121222\n",
      "P(n| ) = 0.0230972\n",
      "P(a| n) = 0.139695\n",
      "P(t| na) = 0.000203246\n",
      "P(i| nat) = 0.0369919\n",
      "------------------------- \n",
      " P( nati) = 0.199986971988\n",
      "-------------------------\n",
      "P( |START) = 0.0121222\n",
      "P(p| ) = 0.00040502\n",
      "P(e| p) = 0.42182\n",
      "P(r| pe) = 0.0043685\n",
      "P(f| per) = 1.88513e-06\n",
      "------------------------- \n",
      " P( perf) = 0.42659554114\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "logProbs_eng = getlogprobs(x_test_subs, prediction_eng, char_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then repeat the process using the LSTM trained with French text, and output the resulting conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_frn = model_frn.predict(x_test_subs_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P( |START) = 0.00197321\n",
      "P(n| ) = 0.00818693\n",
      "P(a| n) = 0.245834\n",
      "P(t| na) = 0.00158867\n",
      "P(i| nat) = 2.77115e-06\n",
      "------------------------- \n",
      " P( nati) = 0.255612757638\n",
      "-------------------------\n",
      "P( |START) = 0.00197321\n",
      "P(p| ) = 0.000842998\n",
      "P(e| p) = 0.047969\n",
      "P(r| pe) = 0.000342633\n",
      "P(f| per) = 7.30374e-07\n",
      "------------------------- \n",
      " P( perf) = 0.0491553147573\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "logProbs_frn = getlogprobs(x_test_subs, prediction_frn, char_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to evaluate the results. Our chosen scoring metric is the ratio of the two log likelihoods. Therefore, for a given string, we have *score = log(Pr(string|english)) - log(Pr(string|french))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = np.array(logProbs_eng) - np.array(logProbs_frn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "AUC = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting ROC curve shows us an AUC score of 0.6514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGHCAYAAACJeOnXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wMedFKrO0g5mwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13424c750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, linewidth=3, color='orange',label='(AUC = %0.4f)' % AUC)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results of the ROC, we are not precisely ecstatic. An AUC of 0.65 is not immediately impressive, and while neither is an accuracy of 60%, it is certainly a great improvement over chance. Upon second review, we begin to realize that for a series of reasons, this classification task went suprisingly well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider the size of the datasets which we used to train the LSTMs. With only ~9,000 characters per model, the corpus sizes are extremely small by most standards. Throughout research and literature, it is recommended for the corpus to have at least 100k characters, if not 1M. With this in mind, the comparatively tiny data size we used to train the models yielded fairly impressive results. Therefore, given a much larger text corpus, we could expect the efficacy of this model to improve significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alteration to the experiment that could improve results is the number of epochs we used to train the LSTM models. While we used only 5, many character-sequential prediction modelelers recommend using at least 20. With a 4-fold increase in training epochs, we may expect to see the LSTM output probability distributions closer to the language's true distribution. This means that the log probability of a string would be a better discriminator of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third potential improvement might be increasing the *maxlen* parameter, which slices the text into substrings of length 5, in this experiment. By increasing this number to something like 50, we would capture much more of the string's context, allowing the LSTM to better learn language-specific patterns and sequences of characters. We could then expect the difference between the two model's  probability estimates to increase for a given string, making it easier to discern the language it belongs to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also try changing the LSTM model paramters. One possibility is to use a sigmoid activation function instead of a softmax. Many other LSTM experiments have been found to work optimally using sigmoid, and while improvement is not guaranteed, it provides us with the possibility of improvement. A similar argument can be made for experimenting with varios loss functions, such as binary cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our aforementioned wishes to increase corpus size as well as number of epochs, we need be mindful of the computational resources they would require. Given the notorious reputation RNNs have for being computationally intensive, we would need access to more powerful hardware if we are to improve our classification accuracy. Therefore, using a GPU and at least 16GB RAM would be the bare minimum. These resources would allow us to conduct larger-scale experimentation that would almost certainly improve our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting alternative to language detection would be applying LSTMs to generate a sound or frequency after being trained with samples of music. Given the intricacy and compliexities of musical composition, it would be quite intriguing to hear what kind of \"music\" would be generated by a LSTM network. One could envision training with various genres and seeing which are easiest to mimic. This could aslo be attempted using musical notes instead of sound frequencies (what would be the difference?). The downside is that the model will likely fall well short of anything resembling human creativity and ingenuity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along a similar path, LSTMs could be used for speech recognition and prediction. This application could be especially useful in mending audio speech recordings with interruptions or dropped signals. An incomplete phrase could then be completed using the model to predict the set of words most likely to come. This could be used in recovering outdated audio recordings and any audio with noise. A difficulty, however, may be finding enough data to train the model with given the specific domain in which it is being applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone has probably already tried it, but applying LSTMs to the stock market might yield interesting results. This type of RNN is a specialist at learning from sequences and time series, and more than often the financial markets offer a nearly unsolvable problem in this setting. Given a sequence of prices over time and a set of factors such as economic indicators, a LSTM may be trained to predict the next most probable price level. Applying this to both long and short term horizons could lead to a potentially new form of pattern recognition in this domain. However, it would be extremely difficult to model all factors, especially those which are intangible or have no quantitative measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this examination of Long Short Term Memory networks, we became familiarized with how this type of RNN functions and what types of tasks it excels at. We then trained two LSTM models with English and French text in an effort to build a language classifier. Although the results were not as glamorous as desired, a list of potential improvements helped rationalize what we observed, and outlined how we could achieve better results. Lastly, some alternative applications of LSTMs were discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
